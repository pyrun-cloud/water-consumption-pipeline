{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cloudbutton Geospatial: Water Consumption Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:25:24.015057Z",
     "start_time": "2021-04-07T09:25:24.010199Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from cloudbutton_geospatial.io_utils.plot import plot_results\n",
    "from cloudbutton_geospatial.utils.notebook import date_picker\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from shapely.geometry import Point, MultiPoint, box\n",
    "from pprint import pprint\n",
    "import functools\n",
    "import collections\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lithops\n",
    "import requests\n",
    "import rasterio\n",
    "import fiona\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "import concurrent.futures\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from lithops.storage import Storage\n",
    "from lithops.storage.utils import StorageNoSuchKeyError\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "from cloud_data_cockpit import DataCockpit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area outside the processed tile that we want to consider for taking SIAM stations into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:28.795793Z",
     "start_time": "2021-04-13T14:38:28.788173Z"
    }
   },
   "outputs": [],
   "source": [
    "AREA_OF_INFLUENCE = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lithops Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:29.658886Z",
     "start_time": "2021-04-13T14:38:29.654251Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_BUCKET = 'pyrundemo'\n",
    "COMPUTE_BACKEND = 'aws_lambda'\n",
    "STORAGE_BACKEND = 'aws_s3'\n",
    "STORAGE_PREFIX = 's3://'\n",
    "RUNTIME_MEMORY = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = lithops.storage.Storage(backend=STORAGE_BACKEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataCockpit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = data_loader.get_data_slices()\n",
    "SPLITS = data_loader.get_batch_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTM_PREFIX = 'DTMs/'\n",
    "DTM_ASC_PREFIX = 'DTMs/asc/'\n",
    "DTM_GEOTIFF_PREFIX = 'DTMs/chunks/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split tile into square chunks (number of tiles = SPLITS^2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation coefficient between elevation and temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:31.362634Z",
     "start_time": "2021-04-13T14:38:31.359578Z"
    }
   },
   "outputs": [],
   "source": [
    "r = -0.0056"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elevation to interpolate temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:32.187402Z",
     "start_time": "2021-04-13T14:38:32.184798Z"
    }
   },
   "outputs": [],
   "source": [
    "zdet = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day of year to calculate solar irradiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date_picker(default=datetime.date(2022, 5, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:33.304420Z",
     "start_time": "2021-04-13T14:38:33.299098Z"
    }
   },
   "outputs": [],
   "source": [
    "DAY_OF_YEAR = date.value.timetuple().tm_yday\n",
    "DAY_OF_YEAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Lithops Storage and Function Executor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fexec = lithops.FunctionExecutor(backend=COMPUTE_BACKEND, storage=STORAGE_BACKEND, runtime_memory=RUNTIME_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIAM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:46:49.948481Z",
     "start_time": "2021-04-07T09:46:49.800147Z"
    }
   },
   "outputs": [],
   "source": [
    "siam_data_key = 'siam_data.csv'\n",
    "try:\n",
    "    siam_data_head = storage.head_object(bucket=DATA_BUCKET, key=siam_data_key)\n",
    "    print(f'SIAM meteo data already in storage: {siam_data_head}')\n",
    "except StorageNoSuchKeyError:\n",
    "    print('Uploading SIAM meteo data to Object Storage...')\n",
    "    with open(siam_data_key, 'rb') as f:\n",
    "        storage.put_object(bucket=DATA_BUCKET, key=siam_data_key, body=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_key = 'shapefile_murcia.zip'\n",
    "try:\n",
    "    shapefile_head = storage.head_object(bucket=DATA_BUCKET, key=shapefile_key)\n",
    "    print(f'Shapefile already in storage: {siam_data_head}')\n",
    "except StorageNoSuchKeyError:\n",
    "    print('Uploading shapefile to Object Storage...')\n",
    "    with open(shapefile_key, 'rb') as f:\n",
    "        storage.put_object(bucket=DATA_BUCKET, key=shapefile_key, body=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download DTM files for free from http://centrodedescargas.cnig.es/CentroDescargas/buscadorCatalogo.do?codFamilia=MDT05# and put them in `input_DTMs` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raster Data Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data tiles in subtiles for increased parallelism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:54.462039Z",
     "start_time": "2021-04-13T14:38:54.449706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_solar_irradiation(inputFile, outputFile, crs='32630'):\n",
    "    # Define grass working set\n",
    "    GRASS_GISDB = '/tmp/grassdata'\n",
    "    #GRASS_GISDB = 'grassdata'\n",
    "    GRASS_LOCATION = 'GEOPROCESSING'\n",
    "    GRASS_MAPSET = 'PERMANENT'\n",
    "    GRASS_ELEVATIONS_FILENAME = 'ELEVATIONS'\n",
    "\n",
    "    os.environ['GRASSBIN'] = 'grass76'\n",
    "\n",
    "    from grass_session import Session\n",
    "    import grass.script as gscript\n",
    "    import grass.script.setup as gsetup\n",
    "    from grass.pygrass.modules.shortcuts import general\n",
    "    from grass.pygrass.modules.shortcuts import raster\n",
    "    \n",
    "    os.environ.update(dict(GRASS_COMPRESS_NULLS='1'))\n",
    "\n",
    "    # Clean previously processed data\n",
    "    if os.path.isdir(GRASS_GISDB):\n",
    "        shutil.rmtree(GRASS_GISDB)\n",
    "    \n",
    "    with Session(gisdb=GRASS_GISDB, location=GRASS_LOCATION, mapset=GRASS_MAPSET, create_opts='EPSG:32630') as ses:\n",
    "        # Set project projection to match elevation raster projection\n",
    "        general.proj(epsg=crs, flags='c') \n",
    "        # Load raster file into working directory\n",
    "        raster.import_(input=inputFile, output=GRASS_ELEVATIONS_FILENAME, flags='o')    \n",
    "        \n",
    "        # Set project region to match raster region\n",
    "        general.region(raster=GRASS_ELEVATIONS_FILENAME, flags='s')    \n",
    "        # Calculate solar irradiation\n",
    "        gscript.run_command('r.slope.aspect', elevation=GRASS_ELEVATIONS_FILENAME,\n",
    "                            slope='slope', aspect='aspect')\n",
    "        gscript.run_command('r.sun', elevation=GRASS_ELEVATIONS_FILENAME,\n",
    "                            slope='slope', aspect='aspect', beam_rad='beam',\n",
    "                            step=1, day=DAY_OF_YEAR)\n",
    "        \n",
    "        # Get extraterrestrial irradiation from history metadata\n",
    "        regex = re.compile(r'\\d+\\.\\d+')\n",
    "        output = gscript.read_command(\"r.info\", flags=\"h\", map=[\"beam\"])\n",
    "        splits = str(output).split('\\n')\n",
    "        line = next(filter(lambda line: 'Extraterrestrial' in line, splits))\n",
    "        extraterrestrial_irradiance = float(regex.search(line)[0])\n",
    "        \n",
    "        # Export generated results into a GeoTiff file\n",
    "        if os.path.isfile(outputFile):\n",
    "            os.remove(outputFile)\n",
    "\n",
    "        raster.out_gdal(input='beam', output=outputFile)\n",
    "        \n",
    "        return extraterrestrial_irradiance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stations contained in the area of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:55.461635Z",
     "start_time": "2021-04-13T14:38:55.457230Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_stations(bounds, stations):\n",
    "    total_points = MultiPoint([Point(x, y) for x, y in stations[['X', 'Y']].to_numpy()])\n",
    "    total_points_list = list(total_points.geoms)\n",
    "    intersection = bounds.buffer(AREA_OF_INFLUENCE).intersection(total_points)\n",
    "    filtered_stations = [point for point in total_points_list if intersection.contains(point)]\n",
    "\n",
    "    return stations[[point in filtered_stations for point in total_points_list]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Distance Weighting interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:58.209269Z",
     "start_time": "2021-04-13T14:38:58.202597Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_basic_interpolation(shape, stations, field_value, offset = (0,0)):\n",
    "    station_pixels = [[pixel[0], pixel[1]] for pixel in stations['pixel'].to_numpy()]\n",
    "    \n",
    "    # Get an array where each position represents pixel coordinates\n",
    "    tile_pixels = np.indices(shape).transpose(1,2,0).reshape(shape[0]*shape[1], 2) + offset\n",
    "    dist = distance_matrix(station_pixels, tile_pixels)\n",
    "    weights = np.where(dist == 0, np.finfo('float32').max, 1.0 / dist )\n",
    "    weights /=  weights.sum(axis=0)\n",
    "    \n",
    "    return np.dot(weights.T, stations[field_value].to_numpy()).reshape(shape).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate temperatures from a subset of the tile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:59.031150Z",
     "start_time": "2021-04-13T14:38:59.005158Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "def radiation_interpolation(\n",
    "    tile_key: str,\n",
    "    block_x: int,\n",
    "    block_y: int,\n",
    "    chunk_slice,\n",
    "    storage\n",
    ") -> list[tuple]:\n",
    "    \"\"\"\n",
    "    For a given COG slice, compute:\n",
    "     - extraterrestrial irradiation raster\n",
    "     - beam radiation raster\n",
    "    Returns two entries:\n",
    "      (tile_key, 'extr', block_x, block_y, extr_cloudobject)\n",
    "      (tile_key, 'rad',  block_x, block_y, rad_cloudobject)\n",
    "    \"\"\"\n",
    "    tile_id, _ = os.path.splitext(tile_key)\n",
    "\n",
    "    # 1) Write the slice to a temporary GeoTIFF\n",
    "    chunk_file = os.path.join(\n",
    "        tempfile.gettempdir(),\n",
    "        f\"{tile_id}_{block_x}_{block_y}.tif\"\n",
    "    )\n",
    "    chunk_slice.to_file(chunk_file)\n",
    "\n",
    "    # 2) Open that chunk to get elevation data and profile\n",
    "    with rasterio.open(chunk_file) as src:\n",
    "        elevation = src.read(1)\n",
    "        profile   = src.profile.copy()\n",
    "        height, width = elevation.shape\n",
    "\n",
    "    # 3) Update profile transform, size, dtype\n",
    "    #    (transform already baked into to_file output)\n",
    "    profile.update({\n",
    "        \"height\": height,\n",
    "        \"width\":  width,\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"dtype\":  elevation.dtype,\n",
    "    })\n",
    "\n",
    "    # 4) Paths for output rasters\n",
    "    extr_path = os.path.join(\n",
    "        tempfile.gettempdir(),\n",
    "        f\"{tile_id}_extr_{block_x}_{block_y}.tif\"\n",
    "    )\n",
    "    rad_path = os.path.join(\n",
    "        tempfile.gettempdir(),\n",
    "        f\"{tile_id}_rad_{block_x}_{block_y}.tif\"\n",
    "    )\n",
    "\n",
    "    # 5) Compute beam radiation via your GRASS/r.sun helper\n",
    "    #    It will write out rad_path and return the \n",
    "    #    extraterrestrial irradiation constant.\n",
    "    extraterrestrial_irradiation = compute_solar_irradiation(\n",
    "        inputFile=chunk_file,\n",
    "        outputFile=rad_path\n",
    "    )\n",
    "\n",
    "    # 6) Write a constant‐value raster for extraterrestrial irradiation\n",
    "    with rasterio.open(extr_path, \"w\", **profile) as dst:\n",
    "        layer = np.full((height, width),\n",
    "                        extraterrestrial_irradiation,\n",
    "                        dtype=elevation.dtype)\n",
    "        dst.write(layer, 1)\n",
    "\n",
    "    # 7) Upload both rasters to object storage\n",
    "    print(extr_path)\n",
    "    print(rad_path)\n",
    "    with open(extr_path, \"rb\") as f:\n",
    "        extr_co = storage.put_cloudobject(body=f, bucket=DATA_BUCKET)\n",
    "\n",
    "    with open(rad_path, \"rb\") as f:\n",
    "        rad_co = storage.put_cloudobject(body=f, bucket=DATA_BUCKET)\n",
    "\n",
    "    return [\n",
    "        (tile_key, \"extr\", block_x, block_y, extr_co),\n",
    "        (tile_key, \"rad\",  block_x, block_y, rad_co),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:39:01.538202Z",
     "start_time": "2021-04-13T14:39:01.521080Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from shapely.geometry import box\n",
    "from lithops.storage import Storage\n",
    "\n",
    "def map_interpolation(\n",
    "    tile_key: str,\n",
    "    block_x: int,\n",
    "    block_y: int,\n",
    "    chunk_slice,\n",
    "    data_field: str\n",
    ") -> list[tuple]:\n",
    "    \"\"\"\n",
    "    Interpolate a meteorological field over one COG slice.\n",
    "    Returns [(tile_key, data_field, block_x, block_y, CloudObject), …].\n",
    "    \"\"\"\n",
    "\n",
    "    # Re-create Storage client inside the worker\n",
    "    storage = Storage(backend=STORAGE_BACKEND)\n",
    "\n",
    "    # 1) Read SIAM stations\n",
    "    siam_stream = storage.get_object(bucket=DATA_BUCKET,\n",
    "                                     key=siam_data_key,\n",
    "                                     stream=True)\n",
    "    siam_data = pd.read_csv(siam_stream)\n",
    "\n",
    "    # 2) Write the single-window chunk to a temp GeoTIFF\n",
    "    tile_id = os.path.splitext(tile_key)[0]\n",
    "    tmp_chunk = os.path.join(tempfile.gettempdir(),\n",
    "                             f\"{tile_id}_{block_x}_{block_y}.tif\")\n",
    "    chunk_slice.to_file(tmp_chunk)\n",
    "\n",
    "    # 3) Open that chunk to get elevation + metadata\n",
    "    with rasterio.open(tmp_chunk) as src:\n",
    "        elevation = src.read(1)\n",
    "        profile   = src.profile.copy()\n",
    "        transform = src.transform\n",
    "        nodata    = src.nodata\n",
    "        height, width = elevation.shape\n",
    "\n",
    "    # 4) Compute the slice’s bounding box and buffer it\n",
    "    minx, miny, maxx, maxy = src.bounds\n",
    "    bbox = box(minx, miny, maxx, maxy).buffer(AREA_OF_INFLUENCE)\n",
    "\n",
    "    # 5) Filter stations inside the buffered bbox\n",
    "    stations = pd.DataFrame(filter_stations(bbox, siam_data))\n",
    "    if stations.empty:\n",
    "        return [(tile_key, data_field, block_x, block_y, None)]\n",
    "\n",
    "    # 6) Convert station coords to pixel indices\n",
    "    stations[\"pixel\"] = stations.apply(\n",
    "        lambda r: rasterio.transform.rowcol(transform, r[\"X\"], r[\"Y\"]),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 7) Prepare output filename & profile (already correct size/transform)\n",
    "    out_file = os.path.join(\n",
    "        tempfile.gettempdir(),\n",
    "        f\"{tile_id}_{data_field}_{block_x}_{block_y}.tif\"\n",
    "    )\n",
    "\n",
    "    # 8) Perform the interpolation\n",
    "    with rasterio.open(out_file, \"w\", **profile) as dst:\n",
    "        if data_field == \"temp\":\n",
    "            interp = compute_basic_interpolation(elevation.shape,\n",
    "                                                 stations,\n",
    "                                                 \"tdet\",\n",
    "                                                 (0, 0))\n",
    "            interp += r * (elevation - zdet)\n",
    "            layer = np.where(elevation == nodata, np.nan, interp)\n",
    "        elif data_field == \"humi\":\n",
    "            layer = compute_basic_interpolation((height, width),\n",
    "                                                stations,\n",
    "                                                \"hr\",\n",
    "                                                (0, 0))\n",
    "        elif data_field == \"wind\":\n",
    "            layer = compute_basic_interpolation((height, width),\n",
    "                                                stations,\n",
    "                                                \"v\",\n",
    "                                                (0, 0))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown data_field {data_field!r}\")\n",
    "\n",
    "        dst.write(layer.astype(profile[\"dtype\"]), 1)\n",
    "\n",
    "    # 9) Upload result and return\n",
    "    print(out_file)\n",
    "    with open(out_file, \"rb\") as f:\n",
    "        co = storage.put_cloudobject(body=f, bucket=DATA_BUCKET)\n",
    "\n",
    "    return [(tile_key, data_field, block_x, block_y, co)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def generate_iterdata(chunks) -> List[Tuple]:\n",
    "    \"\"\"Generates the iterdata array with the data blocks extracted from the COG.\"\"\"\n",
    "    iterdata = []\n",
    "    \n",
    "    for i, window in enumerate(chunks):\n",
    "        tile_key = window.tile_key\n",
    "        block_x = window.block_x\n",
    "        block_y = window.block_y\n",
    "        chunk_data = window\n",
    "\n",
    "        iterdata.append((tile_key, block_x, block_y, chunk_data))\n",
    "    \n",
    "    return iterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata = generate_iterdata(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lithops serverless computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T06:56:29.270356Z",
     "start_time": "2021-04-08T06:56:27.066344Z"
    }
   },
   "outputs": [],
   "source": [
    "res_rad = fexec.map(radiation_interpolation, iterdata, runtime_memory=2048).get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_temp = fexec.map(map_interpolation, iterdata, extra_args=('temp', ), runtime_memory=2048).get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_humi = fexec.map(map_interpolation, iterdata, extra_args=('humi', ), runtime_memory=2048).get_result()\n",
    "res_wind = fexec.map(map_interpolation, iterdata, extra_args=('wind', ), runtime_memory=2048).get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_flatten = []\n",
    "for l in [res_rad, res_temp, res_humi, res_wind]:\n",
    "    for elem in l:\n",
    "        for sub_elem in elem:\n",
    "            print(sub_elem)\n",
    "            res_flatten.append(sub_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_chunks = collections.defaultdict(list)\n",
    "\n",
    "for chunk_result in res_flatten:\n",
    "    tile_key, data_field, block_x, block_y, co = chunk_result\n",
    "    grouped_chunks[(tile_key, data_field)].append((block_x, block_y, co))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join split subsets into a tile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import boto3\n",
    "import tempfile\n",
    "from io import BytesIO\n",
    "\n",
    "import rasterio\n",
    "from affine import Affine\n",
    "from rasterio.windows import Window\n",
    "\n",
    "from dataplug.cloudobject import CloudObject\n",
    "from dataplug.formats.geospatial.cog import CloudOptimizedGeoTiff\n",
    "\n",
    "def merge_blocks(tile_data, chunks, storage):\n",
    "    \"\"\"\n",
    "    tile_data: (tile_key, data_field)\n",
    "    chunks:    [(block_x, block_y, chunk_co), ...]\n",
    "    storage:   Lithops Storage client\n",
    "    \"\"\"\n",
    "    tile_key, data_field = tile_data\n",
    "\n",
    "    session = boto3.Session()\n",
    "    creds = session.get_credentials().get_frozen_credentials()\n",
    "    s3_config = {\n",
    "        \"credentials\": {\n",
    "            \"AccessKeyId\": creds.access_key,\n",
    "            \"SecretAccessKey\": creds.secret_key,\n",
    "            \"SessionToken\": creds.token,\n",
    "        },\n",
    "        \"region_name\": session.region_name,\n",
    "    }\n",
    "\n",
    "    # 1) Load the original COG metadata (no full download)\n",
    "    source_uri = f\"s3://{DATA_BUCKET}/{tile_key}\"\n",
    "    print(source_uri)\n",
    "    orig_co    = CloudObject.from_s3(CloudOptimizedGeoTiff, source_uri, s3_config=s3_config)\n",
    "    attrs      = orig_co.attributes\n",
    "\n",
    "    width, height = attrs.width, attrs.height\n",
    "    base_tf       = Affine(*attrs.transform[:6])  # six values only\n",
    "\n",
    "    # 2) Seed a profile from the first chunk’s profile\n",
    "    bx0, by0, first_co = chunks[0]\n",
    "    first_bytes        = storage.get_cloudobject(first_co)\n",
    "    with rasterio.open(BytesIO(first_bytes)) as src:\n",
    "        profile = src.profile.copy()\n",
    "\n",
    "    # 3) Update to full‐tile size & transform\n",
    "    profile.update({\n",
    "        \"width\":     width,\n",
    "        \"height\":    height,\n",
    "        \"transform\": base_tf,\n",
    "        # keep driver, dtype, nodata, count from the chunk\n",
    "    })\n",
    "\n",
    "    # 4) Create an empty output file\n",
    "    merged_file = os.path.join(\n",
    "        tempfile.gettempdir(),\n",
    "        f\"{data_field}_{os.path.splitext(tile_key)[0]}.tif\"\n",
    "    )\n",
    "    with rasterio.open(merged_file, \"w\", **profile) as dest:\n",
    "        # compute step sizes\n",
    "        step_w = math.floor(width  / SPLITS)\n",
    "        step_h = math.floor(height / SPLITS)\n",
    "\n",
    "        for block_x, block_y, chunk_co in chunks:\n",
    "            # 5) Read only that small chunk\n",
    "            chunk_bytes = storage.get_cloudobject(chunk_co)\n",
    "            with rasterio.open(BytesIO(chunk_bytes)) as src:\n",
    "                arr = src.read(1)\n",
    "                h, w = arr.shape\n",
    "\n",
    "            # 6) Compute window in the big tile\n",
    "            col_off = block_y * step_w\n",
    "            row_off = block_x * step_h\n",
    "            window  = Window(col_off, row_off, w, h)\n",
    "\n",
    "            # 7) Write it in\n",
    "            dest.write(arr, 1, window=window)\n",
    "\n",
    "    # 8) Upload final merged tile\n",
    "    output_key = os.path.join(DTM_PREFIX, data_field, tile_key)\n",
    "    with open(merged_file, \"rb\") as f:\n",
    "        storage.put_object(bucket=DATA_BUCKET, key=output_key, body=f)\n",
    "\n",
    "    return output_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine previous split subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata = []\n",
    "for (tile_id, data_field), chunks in grouped_chunks.items():\n",
    "    iterdata.append(((tile_id, data_field), chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_merged = fexec.map(merge_blocks, iterdata, runtime_memory=4096).get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_keys_merged = set([os.path.basename(t) for t in tiles_merged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile_keys_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of potential evaporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:51.493674Z",
     "start_time": "2021-04-13T16:57:51.485032Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_crop_evapotranspiration(temperatures,\n",
    "                                    humidities,\n",
    "                                    wind_speeds,\n",
    "                                    external_radiations,\n",
    "                                    global_radiations,\n",
    "                                    KCs):\n",
    "    gamma = 0.665*101.3/1000\n",
    "    eSat = 0.6108 * np.exp((17.27*temperatures)/(temperatures+237.3))\n",
    "    delta = 4098 * eSat / np.power((temperatures + 237.3),2)\n",
    "    eA = np.where(humidities < 0, 0, eSat * humidities / 100)     # Avoid sqrt of a negative number\n",
    "    T4 = 4.903 * np.power((273.3 + temperatures),4)/1000000000\n",
    "    rSrS0 = global_radiations/(external_radiations * 0.75)\n",
    "    rN = 0.8* global_radiations-T4*(0.34-0.14*np.sqrt(eA))*((1.35*rSrS0)-0.35)\n",
    "    den = delta + gamma *(1 + 0.34* wind_speeds)\n",
    "    tRad = 0.408 * delta * rN / den\n",
    "    tAdv = gamma * (900/(temperatures+273))*wind_speeds * (eSat - eA)/den\n",
    "    return ((tRad + tAdv) * 7 * KCs).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:52.760441Z",
     "start_time": "2021-04-13T16:57:52.753812Z"
    }
   },
   "outputs": [],
   "source": [
    "vineyard = ['VI', 'VO', 'VF', 'FV', 'CV' ]\n",
    "olive_grove = ['OV', 'VO', 'OF', 'FL', 'OC']\n",
    "fruit = ['FY', 'VF', 'OF', 'FF', 'CF']\n",
    "nuts = ['FS', 'FV', 'FL', 'FF', 'CS' ]\n",
    "citrus = ['CI', 'CV', 'OC', 'CF', 'CS' ]\n",
    "\n",
    "def get_kc(feature):\n",
    "    # TODO: Get more precise values of Kc\n",
    "    print(feature['properties'].keys())\n",
    "    # sigpac_use = feature['properties']['uso_sigpac']\n",
    "    sigpac_use = 'FF'\n",
    "    if sigpac_use in vineyard:\n",
    "        # Grapes for wine - 0.3, 0.7, 0.45\n",
    "        return 0.7  \n",
    "    if sigpac_use in olive_grove:\n",
    "        # Olive grove - ini: 0.65, med: 0.7, end: 0.7\n",
    "        return 0.7 \n",
    "    if sigpac_use in fruit:\n",
    "        # Apples, Cherries, Pears - 0.45, 0.95, 0.7\n",
    "        return 0.95\n",
    "    if sigpac_use in nuts:\n",
    "        # Almonds - 0.4, 0.9, 0.65\n",
    "        return 0.9\n",
    "    if sigpac_use in citrus:\n",
    "        # Citrus, without ground coverage - 0.7, 0.65, 0.7\n",
    "        return 0.65\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:54.250115Z",
     "start_time": "2021-04-13T16:57:54.243932Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_geometry_window(src, geom_bounds):\n",
    "    left, bottom, right, top = geom_bounds\n",
    "    src_left, src_bottom, src_right, src_top = src.bounds\n",
    "    window = src.window(max(left,src_left), max(bottom,src_bottom), min(right,src_right), min(top,src_top))\n",
    "    window_floored = window.round_offsets(op='floor', pixel_precision=3)\n",
    "    w = math.ceil(window.width + window.col_off - window_floored.col_off)\n",
    "    h = math.ceil(window.height + window.row_off - window_floored.row_off)\n",
    "    return Window(window_floored.col_off, window_floored.row_off, w, h)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:57.781920Z",
     "start_time": "2021-04-13T16:57:57.770029Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_evapotranspiration_by_shape(tem, hum, win, rad, extrad, dst):\n",
    "\n",
    "    import fiona\n",
    "    from shapely.geometry import shape, box\n",
    "    from rasterio import features\n",
    "\n",
    "    non_arable_land = ['AG', 'CA', 'ED', 'FO', 'IM', 'PA', 'PR', 'ZU', 'ZV']\n",
    "\n",
    "    #with fiona.open('zip://home/docker/shape.zip') as shape_src:\n",
    "    with fiona.open('zip:///tmp/shape.zip') as shape_src:\n",
    "        for feature in shape_src.filter(bbox=tem.bounds):\n",
    "            KC = get_kc(feature)\n",
    "            if KC is not None:\n",
    "                geom = shape(feature['geometry'])\n",
    "                window = get_geometry_window(tem, geom.bounds)\n",
    "                win_transform = rasterio.windows.transform(window, tem.transform)\n",
    "                # Convert shape to raster matrix\n",
    "                image = features.rasterize([geom],\n",
    "                                           out_shape=(window.height, window.width),\n",
    "                                           transform = win_transform,\n",
    "                                           fill = 0,\n",
    "                                           default_value = 1).astype('bool')\n",
    "                # Get values to compute evapotranspiration\n",
    "                temperatures = tem.read(1, window=window)\n",
    "                humidities = hum.read(1, window=window)\n",
    "                wind_speeds = win.read(1, window=window)\n",
    "                # Convert from W to MJ (0.0036)\n",
    "                global_radiations = rad.read(1, window=window) * 0.0036\n",
    "                external_radiations = extrad.read(1, window=window) * 0.0036\n",
    "                KCs = np.full(temperatures.shape, KC)\n",
    "                # TODO: compute external radiation\n",
    "                #external_radiations = np.full(temperatures.shape, 14)\n",
    "                # TODO: compute global radiation\n",
    "                # global_radiations = np.full(temperatures.shape, 10)\n",
    "                etc = compute_crop_evapotranspiration(\n",
    "                        temperatures,\n",
    "                        humidities,\n",
    "                        wind_speeds,\n",
    "                        external_radiations,\n",
    "                        global_radiations,\n",
    "                        KCs\n",
    "                )\n",
    "                etc[temperatures == tem.nodata] = dst.nodata\n",
    "                etc[np.logical_not(image)] = dst.nodata\n",
    "                dst.write(etc + dst.read(1, window=window), 1, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:59.216824Z",
     "start_time": "2021-04-13T16:57:59.207435Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_global_evapotranspiration(tem, hum, win, rad, extrad, dst):    \n",
    "    for ji, window in tem.block_windows(1):\n",
    "        bounds = rasterio.windows.bounds(window, tem.transform)\n",
    "        temperatures = tem.read(1, window=window)\n",
    "        humidities = hum.read(1, window=window)\n",
    "        wind_speeds = win.read(1, window=window)\n",
    "         # Convert from W to MJ (0.0036)\n",
    "        global_radiations = rad.read(1, window=window) * 0.0036\n",
    "        external_radiations = extrad.read(1, window=window) * 0.0036\n",
    "        # TODO: compute external radiation\n",
    "        #external_radiations = np.full(temperatures.shape, 14)\n",
    "        # TODO: compute global radiation\n",
    "        # global_radiations = np.full(temperatures.shape, 10)\n",
    "        # TODO: compute KCs\n",
    "        KCs = np.full(temperatures.shape, 1)\n",
    "        etc = compute_crop_evapotranspiration(\n",
    "                temperatures,\n",
    "                humidities,\n",
    "                wind_speeds,\n",
    "                external_radiations,\n",
    "                global_radiations,\n",
    "                KCs\n",
    "        )\n",
    "        dst.write(np.where(temperatures == tem.nodata, dst.nodata, etc), 1, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:58:01.128416Z",
     "start_time": "2021-04-13T16:58:01.101842Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_calculations(tile_key, storage):\n",
    "    from functools import partial\n",
    "      \n",
    "    # Download shapefile\n",
    "    shapefile = storage.get_object(bucket=DATA_BUCKET, key='shapefile_murcia.zip', stream=True)\n",
    "\n",
    "    with open('/tmp/shape.zip', 'wb') as shapf:\n",
    "        for chunk in iter(partial(shapefile.read, 200 * 1024 * 1024), ''):\n",
    "            if not chunk:\n",
    "                break\n",
    "            shapf.write(chunk)\n",
    "    try:\n",
    "        temp = storage.get_object(bucket=DATA_BUCKET, key=os.path.join(DTM_PREFIX, 'temp', tile_key))\n",
    "        humi = storage.get_object(bucket=DATA_BUCKET, key=os.path.join(DTM_PREFIX, 'humi', tile_key))\n",
    "        rad = storage.get_object(bucket=DATA_BUCKET, key=os.path.join(DTM_PREFIX, 'rad', tile_key))\n",
    "        extrad = storage.get_object(bucket=DATA_BUCKET, key=os.path.join(DTM_PREFIX, 'extr', tile_key))\n",
    "        wind = storage.get_object(bucket=DATA_BUCKET, key=os.path.join(DTM_PREFIX, 'wind', tile_key))\n",
    "    except StorageNoSuchKeyError:\n",
    "        print(\"Storage error\")\n",
    "        return None\n",
    "    \n",
    "    output_file = os.path.join(tempfile.gettempdir(), 'eva' + '_' + tile_key)\n",
    "    with rasterio.open(BytesIO(temp)) as temp_raster:\n",
    "        with rasterio.open(BytesIO(humi)) as humi_raster:\n",
    "            with rasterio.open(BytesIO(rad)) as rad_raster:\n",
    "                with rasterio.open(BytesIO(extrad)) as extrad_raster:\n",
    "                    with rasterio.open(BytesIO(wind)) as wind_raster:\n",
    "                        profile = temp_raster.profile\n",
    "                        profile.update(nodata=0)\n",
    "                        with rasterio.open(output_file, 'w+', **profile) as dst:\n",
    "#                             compute_global_evapotranspiration(temp_raster, humi_raster, wind_raster,\n",
    "#                                                               rad_raster, extrad_raster, dst)\n",
    "                            compute_evapotranspiration_by_shape(temp_raster, humi_raster, wind_raster,\n",
    "                                                                rad_raster, extrad_raster, dst)\n",
    "    \n",
    "    output_key = os.path.join(DTM_PREFIX, 'eva', tile_key)\n",
    "    with open(output_file, 'rb') as output_f:\n",
    "        storage.put_object(bucket=DATA_BUCKET, key=output_key, body=output_f)\n",
    "    return output_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:58:55.822484Z",
     "start_time": "2021-04-13T16:58:54.943303Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fs_eva = fexec.map(combine_calculations, tile_keys_merged, runtime_memory=2048)\n",
    "res_eva = fexec.get_result(fs=fs_eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_eva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fexec.clean(clean_cloudobjects=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fexec.job_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:32:01.429761Z",
     "start_time": "2021-04-13T21:32:00.245568Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "tile = res_eva[0]\n",
    "tile_key = os.path.basename(tile)\n",
    "tile_id, _ = os.path.splitext(tile_key)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "with rasterio.open(BytesIO(storage.get_object(bucket=DATA_BUCKET, key=tile))) as src:\n",
    "    arr = src.read(1, out_shape=(src.height, src.width))\n",
    "    ax.set_title(tile_id)\n",
    "    img = ax.imshow(arr, cmap='Greens')\n",
    "    fig.colorbar(img, shrink=0.5)\n",
    "\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "# obj.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
